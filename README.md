

---

```markdown
# ğŸ’» AI Laptop Recommendation Assistant

An AI-powered conversational assistant that helps users find the best laptops based on their requirements such as price, RAM, and usage needs (e.g., gaming, office work). It supports follow-up questions and remembers previous context for a personalized experience.

## ğŸ” Features

- Conversational product search assistant for laptops
- Filters by price (under, above, between ranges) and RAM
- Retrieves relevant products using semantic search (FAISS + HuggingFace embeddings)
- Follow-up question support with contextual awareness
- Dual interface: Structured form (Gradio Blocks) and conversational chatbot (Gradio Chatbot)
- Local language model support via Ollama (LLaMA3)
- Memory-enabled dialogue using LangChain

---

## ğŸ› ï¸ Technologies Used

- **LangChain**: Conversational chains, memory, and vector store integrations
- **FAISS**: Fast vector search for similarity-based document retrieval
- **HuggingFace Embeddings**: For semantic similarity
- **Gradio**: User-friendly front-end interface
- **Ollama**: To serve local LLMs like `llama3`
- **Pandas & CSVLoader**: For ingesting and processing laptop data

---

## ğŸ“¦ Folder Structure

```

.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ cleaned\_laptops\_data.csv         # Laptop dataset
â”œâ”€â”€ faiss\_index/                         # Saved FAISS vector index
â”œâ”€â”€ connect\_with\_llm.py                 # Core logic: loading, filtering, chains
â”œâ”€â”€ main\_app.py                         # Chatbot-based Gradio interface
â”œâ”€â”€ app\_with\_blocks.py                  # Form-based Gradio interface
â””â”€â”€ README.md

````

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/ai-laptop-assistant.git
cd ai-laptop-assistant
````

### 2. Install Dependencies

It's recommended to use a virtual environment.

```bash
pip install -r requirements.txt
```

### 3. Start Ollama with LLaMA3 Model

Make sure Ollama is installed and running:

```bash
ollama run llama3
```

### 4. Prepare Data & Build FAISS Index

Run this once to create the vector index:

```bash
python connect_with_llm.py
```

This loads the laptop data, splits documents, generates embeddings, and stores the FAISS index.

### 5. Launch the App

#### Option A: Chatbot Interface

```bash
python main_app.py
```

#### Option B: Form-based Interface with Follow-up Support

```bash
python app_with_blocks.py
```

---

## ğŸ’¡ Example Queries

* "Best laptops under â‚¹60000 with 8GB RAM"
* "Suggest a gaming laptop between â‚¹70000 and â‚¹90000"
* "Above â‚¹80000 with at least 16GB RAM"
* Follow-up: "Which one is light and good for travel?"

---

## ğŸ§  How It Works

1. **User Query** â†’ Parsed for keywords like price, RAM
2. **Retriever** â†’ FAISS fetches relevant laptop entries
3. **Filter** â†’ Additional filtering applied on price/RAM
4. **LLM Prompt** â†’ LLaMA3 generates top 5 recommendations
5. **Follow-up** â†’ Context-aware LLM answers additional questions

---

## âœ… To-Do / Improvements

* âœ… Add price and RAM filtering logic
* âœ… Implement follow-up question handling
* âœ… Contextual memory via LangChain
* ğŸ”„ Add GPU or battery filtering
* ğŸ”„ Improve UI layout and usability
* ğŸ”„ Export recommendations as PDF

---

## ğŸ“„ License

MIT License

---

## ğŸ™ Acknowledgments

* [LangChain](https://github.com/langchain-ai/langchain)
* [Ollama](https://ollama.com)
* [Gradio](https://www.gradio.app/)
* [FAISS](https://github.com/facebookresearch/faiss)
* [HuggingFace Sentence Transformers](https://www.sbert.net/)

```

---

Let me know if you'd like a `requirements.txt` or a sample `cleaned_laptops_data.csv` structure added too.
```

